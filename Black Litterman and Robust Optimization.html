<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Peng Liu">

<title>Robust Portfolio Optimization with Generalized Black–Litterman Model</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="main_files/libs/clipboard/clipboard.min.js"></script>
<script src="main_files/libs/quarto-html/quarto.js"></script>
<script src="main_files/libs/quarto-html/popper.min.js"></script>
<script src="main_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="main_files/libs/quarto-html/anchor.min.js"></script>
<link href="main_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="main_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="main_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="main_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="main_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Robust Portfolio Optimization with Generalized Black–Litterman Model</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Peng Liu </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="main-idea" class="level2">
<h2 class="anchored" data-anchor-id="main-idea">Main Idea</h2>
<p>The integration of the Generalized Black-Litterman (GBL) model with robust optimization enhances portfolio construction by addressing both estimation uncertainty and model misspecification. The GBL model, as proposed by <span class="citation" data-cites="chen2020">Chen and Lim (<a href="#ref-chen2020" role="doc-biblioref">2020</a>)</span>, refines return estimates by accounting for biases in the equilibrium model (such as the CAPM) and expert views using a Bayesian graphical framework. This results in a posterior distribution of asset returns that better reflects the underlying uncertainties.</p>
<p>Instead of using the posterior mean and covariance directly in a standard Mean-Variance Optimization (MVO), we further incorporate robust optimization techniques. By leveraging the distributional information from the GBL model, we define uncertainty sets for asset returns based on confidence intervals derived from the posterior distribution. The portfolio optimization problem is then reformulated to maximize expected return while minimizing risk under the worst-case scenarios within these uncertainty sets. This robust approach provides portfolios that are more resilient to estimation errors and model uncertainties.</p>
</section>
<section id="black-litterman-model" class="level2">
<h2 class="anchored" data-anchor-id="black-litterman-model">Black-Litterman Model</h2>
<p>The Black-Litterman model, first proposed by <span class="citation" data-cites="black1991">(<a href="#ref-black1991" role="doc-biblioref">Black and Litterman 1991</a>)</span>, essentially provides a better returns forecast in a form of an updated posterior normal distribution based on a <strong>prior distribution</strong> (in the form of <strong>capital asset pricing model (CAPM)</strong> as the equilibrium model) and <strong>observed data</strong> (in the form of <strong>expert views</strong>). However, the downside is that the equilibrium model may ignore certain relevant factors, such as size and momentum, thus leading to systematic errors due to potential misspecification. In addition, expert views may also be biased. Such biases are explicitly modeled and learned in the GBL model.</p>
<section id="equilibrium-model-and-prior-distribution-for-returns" class="level3">
<h3 class="anchored" data-anchor-id="equilibrium-model-and-prior-distribution-for-returns">Equilibrium Model and Prior Distribution for Returns</h3>
<p>Let <span class="math inline">\(\mathbf{r}\)</span> denote the <span class="math inline">\(N\)</span>-dimensional vector of returns for the risky assets<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. We assume the following multivariate normal distribution:</p>
<p><span id="eq-priordist"><span class="math display">\[
\mathbf{r} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu}\)</span> is set equal to the expected returns obtained from a backward-looking equilibrium model such as the CAPM, and <span class="math inline">\(\Sigma\)</span> is the covariance matrix of returns either user specified, estimated via sample average, or using other methods.</p>
<p><a href="#eq-priordist" class="quarto-xref">Equation&nbsp;1</a> serves as the prior distribution for the latent return vector <span class="math inline">\(\mathbf{r}\)</span> whose value will only be revealed at the end of the investment period.</p>
</section>
<section id="expert-views" class="level3">
<h3 class="anchored" data-anchor-id="expert-views">Expert Views</h3>
<p>Expert views are solicited to refine the prior distribution of returns in <a href="#eq-priordist" class="quarto-xref">Equation&nbsp;1</a> via the Bayes’ rule. These views serve as a noisy forecast of the latent return vector <span class="math inline">\(\mathbf{r}\)</span> and come in as the likelihood function in the Bayes’ update mechanism.</p>
<p>For the following two sample views on three assets:</p>
<p><em>“Asset 1’s return will be 10%”</em></p>
<p><em>“Asset 2 will outperform asset 3 by 2%”</em></p>
<p>We can write this view vector as</p>
<p><span class="math display">\[
\mathbf{v} = \begin{pmatrix}
v_{1} \\
v_{2-3}
\end{pmatrix}
= \begin{pmatrix}
10\% \\
2\%
\end{pmatrix}
\]</span></p>
<p>Alternatively, we can use a <span class="math inline">\(K \times N\)</span> link matrix <span class="math inline">\(P\)</span> to linearly map the return vector with the view vector:</p>
<p><span class="math display">\[
P\mathbf{r} =
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; -1
\end{pmatrix}
\begin{pmatrix}
r_{1} \\
r_{2} \\
r_{3}
\end{pmatrix}
= \begin{pmatrix}
r_{1} \\
r_{2} - r_{3}
\end{pmatrix}.
\]</span></p>
<p>where <span class="math inline">\(K\)</span> denotes the number of expert views. Observe that rows of the link matrix representing absolute views sum to 1, whereas rows representing relative views sum to 0.</p>
<p>The view vector <span class="math inline">\(\mathbf{v}\)</span>, conditional on the unobserved return vector <span class="math inline">\(\mathbf{r}\)</span>, can be modeled as the following Gaussian distribution:</p>
<p><span class="math display">\[
\mathbf{v} \sim \mathcal{N}(P\mathbf{r}, \Omega)
\]</span></p>
<p>where <span class="math inline">\(\Omega\)</span> specifies the level of confidence in expert views.</p>
</section>
<section id="obtaining-the-posterior-return-distribution" class="level3">
<h3 class="anchored" data-anchor-id="obtaining-the-posterior-return-distribution">Obtaining the Posterior Return Distribution</h3>
<p>The posterior distribution, which turns out to be also normal, can be expressed as:</p>
<p><span id="eq-BLposterior"><span class="math display">\[
\mathbf{r}|\mathbf{v} \sim \mathcal{N}(\boldsymbol{\mu}_{BL}, \Sigma_{BL})
\tag{2}\]</span></span></p>
<p>where <span class="math display">\[
\boldsymbol{\mu}_{BL} = \left( \Sigma^{-1} + P^\top \Omega^{-1} P \right)^{-1} \left( \Sigma^{-1} \boldsymbol{\mu} + P^\top \Omega^{-1} \mathbf{v} \right)
\]</span> <span class="math display">\[
\Sigma_{BL} = \left( \Sigma^{-1} + P^\top \Omega^{-1} P \right)^{-1}
\]</span></p>
<p>This shows that the mean return is an average of the prior mean and the view weighted by the confidence in each, as determined by the inverse of the covariance matrices. We also observe a smaller posterior variance <span class="math inline">\(\Sigma_{BL}\)</span> due to additional information absorbed. Portfolio allocations are then obtained by solving a mean-variance problem with return distribution in <a href="#eq-BLposterior" class="quarto-xref">Equation&nbsp;2</a>.</p>
<p>Also note that the scalar <span class="math inline">\(\tau\)</span>, which is used to scale uncertainty in market equilibrium vector, is dropped in favor of incorporating it in <span class="math inline">\(\Sigma\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Details on Deriving the Posterior Distribution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Starting with the prior distribution and the likelihood of the expert views, we aim to derive the posterior distribution of the returns vector <span class="math inline">\(\mathbf{r}\)</span> given the views <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p>The prior distribution of the returns is given by:</p>
<p><span class="math display">\[
\mathbf{r} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)
\]</span></p>
<p>which implies the probability density function:</p>
<p><span class="math display">\[
p(\mathbf{r}) = \frac{1}{(2\pi)^{N/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{r} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{r} - \boldsymbol{\mu}) \right)
\]</span></p>
<p>The expert views are modeled as:</p>
<p><span class="math display">\[
\mathbf{v} = P\mathbf{r} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \Omega)
\]</span></p>
<p>which implies:</p>
<p><span class="math display">\[
\mathbf{v}|\mathbf{r} \sim \mathcal{N}(P\mathbf{r}, \Omega)
\]</span></p>
<p>The corresponding likelihood function is:</p>
<p><span class="math display">\[
p(\mathbf{v}|\mathbf{r}) = \frac{1}{(2\pi)^{K/2} |\Omega|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{v} - P\mathbf{r})^\top \Omega^{-1} (\mathbf{v} - P\mathbf{r}) \right)
\]</span></p>
<p>Applying Bayes’ theorem:</p>
<p><span class="math display">\[
p(\mathbf{r}|\mathbf{v}) \propto p(\mathbf{v}|\mathbf{r}) p(\mathbf{r})
\]</span></p>
<p>Substituting the expressions for the likelihood and the prior:</p>
<p><span class="math display">\[
p(\mathbf{r}|\mathbf{v}) \propto \exp\left( -\frac{1}{2} (\mathbf{v} - P\mathbf{r})^\top \Omega^{-1} (\mathbf{v} - P\mathbf{r}) - \frac{1}{2} (\mathbf{r} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{r} - \boldsymbol{\mu}) \right)
\]</span></p>
<p>Expanding the exponents:</p>
<p><span class="math display">\[
-\frac{1}{2} (\mathbf{v} - P\mathbf{r})^\top \Omega^{-1} (\mathbf{v} - P\mathbf{r}) - \frac{1}{2} (\mathbf{r} - \boldsymbol{\mu})^\top \Sigma^{-1} (\mathbf{r} - \boldsymbol{\mu}) \] \[ = -\frac{1}{2} \left( \mathbf{v}^\top \Omega^{-1} \mathbf{v} - 2 \mathbf{v}^\top \Omega^{-1} P \mathbf{r} + \mathbf{r}^\top P^\top \Omega^{-1} P \mathbf{r} \right) \] \[ \quad - \frac{1}{2} \left( \mathbf{r}^\top \Sigma^{-1} \mathbf{r} - 2 \mathbf{r}^\top \Sigma^{-1} \boldsymbol{\mu} + \boldsymbol{\mu}^\top \Sigma^{-1} \boldsymbol{\mu} \right) \] \[ = -\frac{1}{2} \mathbf{r}^\top \left( P^\top \Omega^{-1} P + \Sigma^{-1} \right) \mathbf{r} + \mathbf{r}^\top \left( P^\top \Omega^{-1} \mathbf{v} + \Sigma^{-1} \boldsymbol{\mu} \right) + \text{const}
\]</span></p>
<p>To express the posterior in the form of a multivariate normal distribution, we complete the square for <span class="math inline">\(\mathbf{r}\)</span>:</p>
<p><span class="math display">\[
-\frac{1}{2} (\mathbf{r} - \boldsymbol{\mu}_{BL})^\top \Sigma_{BL}^{-1} (\mathbf{r} - \boldsymbol{\mu}_{BL}) + \text{const}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu}_{BL}\)</span> and <span class="math inline">\(\Sigma_{BL}\)</span> are the posterior mean and covariance matrix, respectively.</p>
<p>By matching coefficients, we identify:</p>
<p><span class="math display">\[
\Sigma_{BL}^{-1} = \Sigma^{-1} + P^\top \Omega^{-1} P
\]</span></p>
<p><span class="math display">\[
\Sigma_{BL}^{-1} \mu_{BL} = \Sigma^{-1} \boldsymbol{\mu} + P^\top \Omega^{-1} \mathbf{v}
\]</span></p>
<p>Solving for <span class="math inline">\(\boldsymbol{\mu}_{BL}\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_{BL} = \Sigma_{BL} \left( \Sigma^{-1} \boldsymbol{\mu} + P^\top \Omega^{-1} \mathbf{v} \right)
\]</span></p>
<p>Substituting <span class="math inline">\(\Sigma_{BL}^{-1}\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_{BL} = \left( \Sigma^{-1} + P^\top \Omega^{-1} P \right)^{-1} \left( \Sigma^{-1} \boldsymbol{\mu} + P^\top \Omega^{-1} \mathbf{v} \right)
\]</span></p>
<p>Thus, the posterior distribution is:</p>
<p><span class="math display">\[
\mathbf{r}|\mathbf{v} \sim \mathcal{N}(\mu_{BL}, \Sigma_{BL})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{\mu}_{BL} = \left( \Sigma^{-1} + P^\top \Omega^{-1} P \right)^{-1} \left( \Sigma^{-1} \boldsymbol{\mu} + P^\top \Omega^{-1} \mathbf{v} \right)
\]</span></p>
<p><span class="math display">\[
\Sigma_{BL} = \left( \Sigma^{-1} + P^\top \Omega^{-1} P \right)^{-1}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="optimal-portfolio" class="level3">
<h3 class="anchored" data-anchor-id="optimal-portfolio">Optimal Portfolio</h3>
<p>Based on the updated posterior estimates <span class="math inline">\(\boldsymbol{\mu}_{BL}\)</span> and <span class="math inline">\(\Sigma_{BL}\)</span>, we can determine the optimal portfolio weights <span class="math inline">\(\mathbf{w}^*\)</span> that maximize the investor’s utility, balancing expected return against risk under the risk aversion parameter <span class="math inline">\(\gamma\)</span>:</p>
<p><span class="math display">\[
\max_{\mathbf{w}} \mathbb{E}[\mathbf{r}]^T \mathbf{w} - \frac{\gamma}{2} \mathbf{w}^T \operatorname{Var}(\mathbf{r}) \mathbf{w} = \boldsymbol{\mu}_{BL}^T \mathbf{w} - \frac{\gamma}{2} \mathbf{w}^T \Sigma_{BL} \mathbf{w}
\]</span></p>
<p>The optimal portfolio is given by:</p>
<p><span class="math display">\[
\mathbf{w}^* = \frac{1}{\gamma} \Sigma_{BL}^{-1} \boldsymbol{\mu}_{BL}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deriving the Optimal Portfolio
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To find the optimal weights <span class="math inline">\(\mathbf{w}^*\)</span>, we take the derivative of the utility function with respect to <span class="math inline">\(\mathbf{w}\)</span> and set it to zero.</p>
<p><span class="math display">\[
\frac{\partial U}{\partial \mathbf{w}} = \boldsymbol{\mu}_{BL} - \gamma \Sigma_{BL} \mathbf{w} = \mathbf{0}
\]</span> Rearranging the first-order condition to solve for <span class="math inline">\(\mathbf{w}^*\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_{BL} - \gamma \Sigma_{BL} \mathbf{w}^* = \mathbf{0}
\]</span></p>
<p><span class="math display">\[
\gamma \Sigma_{BL} \mathbf{w}^* = \boldsymbol{\mu}_{BL}
\]</span></p>
<p><span class="math display">\[
\mathbf{w}^* = \frac{1}{\gamma} \Sigma_{BL}^{-1} \boldsymbol{\mu}_{BL}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="graphical-representation" class="level3">
<h3 class="anchored" data-anchor-id="graphical-representation">Graphical Representation</h3>
<p>Both equilibrium model and expert views can be embedded in a Bayesian graph to express their conditional dependence, where the former is unobserved random variables represented by circles and the latter is observed random variables represented by squares. The Bayesian graph of the Black-Litterman model is shown in <a href="#fig-BL" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div id="fig-BL" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-BL-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/BL_graph.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-BL-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Graphical Representation of the Classical Black–Litterman Model
</figcaption>
</figure>
</div>
<p>Note that historical views and returns are not used in this representation. Next, we introduce the GBL model that makes full use of these historical data.</p>
</section>
</section>
<section id="generalized-black-litterman-model" class="level2">
<h2 class="anchored" data-anchor-id="generalized-black-litterman-model">Generalized Black-Litterman Model</h2>
<p>In general, GBL model assumes an additional bias term in both returns and views, where the bias may change over time. Incorporating such biases, which are estimated using historical views and returns, allows us to better estimate future returns vector of the next period.</p>
<p>In addition, GBL makes better use of the multi-period nature of the data, as shown in <a href="#fig-GBL_multiperiod" class="quarto-xref">Figure&nbsp;2</a>.</p>
<div id="fig-GBL_multiperiod" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-GBL_multiperiod-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/GBL_multiperiod.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-GBL_multiperiod-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Graphical Representation of Multi-Period Nature of GBL
</figcaption>
</figure>
</div>
<section id="accounting-for-bias-in-views" class="level3">
<h3 class="anchored" data-anchor-id="accounting-for-bias-in-views">Accounting for Bias in Views</h3>
<p>The classical BL model assumes that views are unbiased noisy observations of the realized return vector. As a remedy, we introduce <span class="math inline">\(b^{\mathbf{v}}_t\)</span> to denote the (unobserved) view bias in time <span class="math inline">\(t\)</span>, and further assume it follows a normal distribution with fixed parameters to be learned via historical data, i.e., <span class="math inline">\(b^{\mathbf{v}}_t \sim \mathcal{N}(\delta^{\mathbf{v}}, \Theta^{\mathbf{v}})\)</span>. Such bias is modelled as an additional shift to the expected return, moving from <span class="math inline">\(P\mathbf{r}_t\)</span> to <span class="math inline">\(P\mathbf{r}_t+b^{\mathbf{v}}_t\)</span>, i.e., <span class="math inline">\(\mathbf{v}_t|\mathbf{r}_t,b_t^{\mathbf{v}} \sim \mathcal{N}(P\mathbf{r}_t+b^{\mathbf{v}}_t, \Omega)\)</span> for each period.</p>
<p>The chain of action is to make use of historical data<span class="math inline">\(\{(\mathbf{v}_1, \mathbf{r}_1), (\mathbf{v}_2, \mathbf{r}_2), \dots, (\mathbf{v}_{t-1}, \mathbf{r}_{t-1})\}\)</span> to better estimate moments <span class="math inline">\((\delta^{\mathbf{v}}, \Theta^{\mathbf{v}})\)</span>, which are then further used to correct for the current period bias <span class="math inline">\(b^{\mathbf{v}}_t\)</span> and improve forecast of future returns (through the current observed view <span class="math inline">\(\mathbf{v}_t\)</span>).</p>
<p>Note that the bias in each period is modeled as an independent draw from a common normal distribution to indicate the fact that experts may refer to the same source of information or exchange information with each other upon forming the views.</p>
<p>We can incorporate the biased view in <a href="#fig-GBL_view" class="quarto-xref">Figure&nbsp;3</a>, where observed quantities are expressed in squares and unobserved random variables in circles.</p>
<div id="fig-GBL_view" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-GBL_view-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/GBL_view.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-GBL_view-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Graphical Representation of Generalized Black–Litterman Accounting for Expert View Bias
</figcaption>
</figure>
</div>
</section>
<section id="accounting-for-bias-in-returns" class="level3">
<h3 class="anchored" data-anchor-id="accounting-for-bias-in-returns">Accounting for Bias in Returns</h3>
<p>The equilibrium model assumed in CAPM may be misspecified and ignore other important factors as discussed in <span class="citation" data-cites="fama2003">(<a href="#ref-fama2003" role="doc-biblioref">Fama and French 2003</a>)</span>. The classisical BL model assigns a fixed expected return <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(\boldsymbol{\mu}_t\)</span> for all periods.</p>
<p>In contrast, the GBL model corrects this by introducing an independent <span class="math inline">\(\boldsymbol{\mu}_t\)</span> for each period, giving <span class="math inline">\(\mathbf{r}_t | \boldsymbol{\mu}_t \sim \mathcal{N}(\boldsymbol{\mu}_t, \Sigma)\)</span>. Each <span class="math inline">\(\boldsymbol{\mu}_t\)</span> is then modeled as independent draws from common normal distribution with an unobserved bias <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span> centered around CAPM estimate <span class="math inline">\(\alpha\)</span>, giving: <span class="math inline">\(\boldsymbol{\mu}_t | b^{\boldsymbol{\mu}} \sim \mathcal{N}(\alpha+b^{\boldsymbol{\mu}},\beta)\)</span>.</p>
<p><a href="#fig-GBL_returns" class="quarto-xref">Figure&nbsp;4</a> shows the returns portion of the GBL model with bias correction.</p>
<div id="fig-GBL_returns" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-GBL_returns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/GBL_returns.png" class="img-fluid quarto-figure quarto-figure-center figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-GBL_returns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Graphical Representation of Generalized Black–Litterman Accounting for Expert View Bias
</figcaption>
</figure>
</div>
</section>
<section id="combined-graphical-model" class="level3">
<h3 class="anchored" data-anchor-id="combined-graphical-model">Combined Graphical Model</h3>
<p>The combined graphical model is shown in <a href="#fig-GBL" class="quarto-xref">Figure&nbsp;5</a>. The eventual goal is to generate a better posterior estimate of <span class="math inline">\(\mathbf{r}_t\)</span>, given the history of observed\realized returns <span class="math inline">\(\mathcal{R}_{t-1}=\{\mathbf{r}_1,\mathbf{r}_2,\dots,\mathbf{r}_{t-1}\}\)</span> until last period <span class="math inline">\(t-1\)</span> and experts views <span class="math inline">\(\mathcal{V}_{t}=\{\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_{t}\}\)</span>, including the current period <span class="math inline">\(t\)</span>. <span class="math inline">\(\mathbf{r}_t\)</span> is only revealed at the end of period <span class="math inline">\(t\)</span>. These quantities are connected via the proposed GBL model, highlighting the unobserved biases in both returns and views.</p>
<div id="fig-GBL" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="#fig-GBL">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-GBL-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/GBL_overall.png" class="img-fluid figure-img" data-fig-env="#fig-GBL">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-GBL-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Graphical Representation of GBL Model
</figcaption>
</figure>
</div>
<p>These biases, together with model parameters, are jointly estimated using the history of returns and expert views.</p>
<p>Let us visit these two components in more detail.</p>
</section>
<section id="equilibrium-model" class="level3">
<h3 class="anchored" data-anchor-id="equilibrium-model">Equilibrium Model</h3>
<p>The classic CAPM in BL model assumes a normally distributed return vector with a fixed/known mean vector <span class="math inline">\(\alpha\)</span>:</p>
<p><span class="math display">\[
\mathbf{r}_t \sim \mathcal{N}(\alpha, \Sigma)
\]</span></p>
<p>which essentially assumes <span class="math inline">\(\boldsymbol{\mu}_t=\alpha\)</span> for all periods. The revised counterpart in GBL assumes a normally distributed mean vector <span class="math inline">\(\boldsymbol{\mu}_t\)</span> centered around <span class="math inline">\(\alpha\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_t | b^{\boldsymbol{\mu}} \sim \mathcal{N}(\alpha+b^{\boldsymbol{\mu}},\beta)
\]</span></p>
<p>where <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span> models the bias in CAPM estimate of expect return, and <span class="math inline">\(\beta\)</span> represents the stochastic factors that change <span class="math inline">\(\boldsymbol{\mu}_t\)</span> over time and is assumed to be known. The covariance matrix <span class="math inline">\(\beta\)</span> can be specified by the user to express the confidence on the coverage of the expected return.</p>
<p>We further model the return bias as:</p>
<p><span class="math display">\[
b^{\boldsymbol{\mu}} \sim \mathcal{N}(\delta_0^{\boldsymbol{\mu}},\Theta_0^{\boldsymbol{\mu}})
\]</span></p>
<p>where parameters <span class="math inline">\(\delta_0^{\boldsymbol{\mu}}\)</span> and <span class="math inline">\(\Theta_0^{\boldsymbol{\mu}}\)</span> are constants and updated in GBL.</p>
</section>
<section id="expert-views-1" class="level3">
<h3 class="anchored" data-anchor-id="expert-views-1">Expert Views</h3>
<p>The classic BL assumes the expert views are unbiased noisy observations of linear functions of returns: <span class="math inline">\(\mathbf{v}_t|\mathbf{r}_t \sim \mathcal{N}(P\mathbf{r}_t, \Omega)\)</span>. The GBL model allowed biased views via:</p>
<p><span class="math display">\[
\mathbf{v}_t|\mathbf{r}_t,b_t^{\mathbf{v}} \sim \mathcal{N}(P\mathbf{r}_t+b^{\mathbf{v}}_t, \Omega)
\]</span></p>
<p>where the unobserved and independently drawn views are modeled as:</p>
<p><span class="math display">\[
b^{\mathbf{v}}_t | \delta^{\mathbf{v}}, \Theta^{\mathbf{v}} \sim \mathcal{N}(\delta^{\mathbf{v}}, \Theta^{\mathbf{v}})
\]</span></p>
<p>where the parameters <span class="math inline">\(\delta^{\mathbf{v}}, \Theta^{\mathbf{v}}\)</span> are constant and follow a normal inverse Wishart distribution:</p>
<p><span class="math display">\[
(\delta^V, \Theta^V) \sim \text{NIW}(\eta_0^V, \kappa_0^V, \nu_0^V, \Lambda_0^V)
\]</span></p>
<p>where the parameters <span class="math inline">\(\eta_0^V, \kappa_0^V, \nu_0^V, \Lambda_0^V\)</span> are updated using the historical views and returns. Note that <span class="math inline">\(\Theta^{\mathbf{v}}\)</span> also encodes the correlation among expert views in its off-diagonal entries.</p>
<p>The updated graphical models is shown in <a href="#fig-GBL2" class="quarto-xref">Figure&nbsp;6</a>.</p>
<div id="fig-GBL2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-GBL2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figs/GBL_overall2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-GBL2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Updated Graphical Representation of GBL Model
</figcaption>
</figure>
</div>
</section>
<section id="asymetric-treatment-of-beta-and-thetamathbfv" class="level3">
<h3 class="anchored" data-anchor-id="asymetric-treatment-of-beta-and-thetamathbfv">Asymetric Treatment of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\Theta^{\mathbf{v}}\)</span></h3>
<p>Note that <span class="math inline">\(\beta\)</span> is assumed to be known and thus specified by the user due to more confidence (and longer history) of returns than expert views. <span class="math inline">\(\Theta^{\mathbf{v}}\)</span> needs to learned from data. Such treatment significantly accelerates computation time.</p>
</section>
<section id="model-summary" class="level3">
<h3 class="anchored" data-anchor-id="model-summary">Model Summary</h3>
<p>We now summarize the type of parameters as follows.</p>
<ul>
<li><p>Parameters that are directly derived using data and remain fixed during estimation:</p>
<ul>
<li>CAPM estimate <span class="math inline">\(\alpha\)</span></li>
</ul></li>
<li><p>Parameters that are user specified and remain fixed during estimation:</p>
<ul>
<li><p>covariance matrix of returns <span class="math inline">\(\Sigma\)</span></p></li>
<li><p>uncertainty in CAPM forecast <span class="math inline">\(\beta\)</span></p></li>
<li><p>expert covariance matrix <span class="math inline">\(\Omega\)</span></p></li>
</ul></li>
</ul>
<p>Besides, we have access to historical data <span class="math inline">\(\mathcal{R}_{t-1}\)</span> and <span class="math inline">\(\mathcal{V}_t\)</span>, and our goal is to estimate the posterior distribution of the return vector <span class="math inline">\(\mathbf{r}_t\)</span>.</p>
<p>Also, note that the top and bottom parameters, including <span class="math inline">\((\delta_0^{\boldsymbol{\mu}},\Theta_0^{\boldsymbol{\mu}})\)</span> and <span class="math inline">\((\eta_0^V, \kappa_0^V, \nu_0^V, \Lambda_0^V)\)</span> are user-specified and updated using historical data <span class="math inline">\(\mathcal{R}_{t-1}\)</span> and <span class="math inline">\(\mathcal{V}_t\)</span>.</p>
<p>The joint posterior distribution of all random variables given observed data can be expressed as:</p>
<p><span class="math display">\[
P(\mathbf{r}_t, b^{\boldsymbol{\mu}},(\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\dots,\boldsymbol{\mu}_t),(b^{\mathbf{v}}_1,b^{\mathbf{v}}_2,\dots,b^{\mathbf{v}}_t),(\delta^V, \Theta^V) | \mathcal{R}_{t-1}, \mathcal{V}_t)
\]</span></p>
<p>where the “tail-end” random variables are specified as:</p>
<p><span class="math display">\[
b^{\boldsymbol{\mu}} \sim \mathcal{N}(\delta_0^{\boldsymbol{\mu}},\Theta_0^{\boldsymbol{\mu}})
\]</span></p>
<p><span class="math display">\[
(\delta^V, \Theta^V) \sim \text{NIW}(\eta_0^V, \kappa_0^V, \nu_0^V, \Lambda_0^V)
\]</span></p>
</section>
</section>
<section id="model-estimation" class="level2">
<h2 class="anchored" data-anchor-id="model-estimation">Model Estimation</h2>
<p>Due to complexity of the graphical model, we use Gibbs sampling to estimate the parameters of joint posterior distribution. In the Generalized Black-Litterman (GBL) model, it’s crucial to distinguish between known parameters, unknown parameters (to be estimated), and observed data. Understanding the conditional dependencies among these elements is essential for accurately deriving the posterior parameters during the Gibbs sampling process.</p>
<section id="known-user-specified-parameters" class="level3">
<h3 class="anchored" data-anchor-id="known-user-specified-parameters"><strong>Known (User-Specified) Parameters</strong></h3>
<p>These parameters are predefined and remain fixed throughout the Gibbs sampling process:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span>: The fixed CAPM estimate of expected returns.</li>
<li><span class="math inline">\(\beta\)</span>: The known covariance matrix representing uncertainty in the CAPM forecast.</li>
<li><span class="math inline">\(\Sigma\)</span>: The covariance matrix of asset returns.</li>
<li><span class="math inline">\(P\)</span>: The <span class="math inline">\(K \times N\)</span> link matrix mapping the latent return vector <span class="math inline">\(\mathbf{r}\)</span> to the expert view vector <span class="math inline">\(\mathbf{v}\)</span>.</li>
<li><span class="math inline">\(\Omega\)</span>: The covariance matrix representing uncertainty in the expert views.</li>
</ul>
</section>
<section id="unknown-latent-parameters" class="level3">
<h3 class="anchored" data-anchor-id="unknown-latent-parameters"><strong>Unknown (Latent) Parameters</strong></h3>
<p>These parameters are not directly observable and need to be estimated:</p>
<ul>
<li><span class="math inline">\(\mathbf{r}_t\)</span>: The <span class="math inline">\(N\)</span>-dimensional latent return vector at time <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(b^{\boldsymbol{\mu}}\)</span>: The bias term in the equilibrium (CAPM) model.</li>
<li><span class="math inline">\(\{\boldsymbol{\mu}_s\}_{s=1}^t\)</span>: The mean return vectors for each period <span class="math inline">\(s = 1, \dots, t\)</span>.</li>
<li><span class="math inline">\(\{b^{\mathbf{v}}_s\}_{s=1}^t\)</span>: The bias terms for the expert views at each period.</li>
<li><span class="math inline">\(\delta^{\mathbf{v}}\)</span> and <span class="math inline">\(\Theta^{\mathbf{v}}\)</span>: Hyperparameters governing the distribution of the view biases <span class="math inline">\(b^{\mathbf{v}}_s\)</span>.</li>
</ul>
</section>
<section id="observed-data" class="level3">
<h3 class="anchored" data-anchor-id="observed-data"><strong>Observed Data</strong></h3>
<p>These are the data points available for parameter estimation:</p>
<ul>
<li><strong>Historical Returns</strong> <span class="math inline">\(\mathcal{R}_{t-1} = \{\mathbf{r}_1, \mathbf{r}_2, \dots, \mathbf{r}_{t-1}\}\)</span>: Realized returns up to time <span class="math inline">\(t-1\)</span>.</li>
<li><strong>Expert Views</strong> <span class="math inline">\(\mathcal{V}_t = \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_t\}\)</span>: Expert-provided views up to time <span class="math inline">\(t\)</span>.</li>
</ul>
<p>In Gibbs sampling, we iteratively sample each unknown parameter from its conditional distribution, given the current values of all other parameters and the observed data. The conditional dependencies among the parameters dictate how these distributions are derived.</p>
<p>See the following for the details of deriving posterior parameters.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deriving Parameters of Posterior Distribution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<section id="a.-sampling-the-return-vector-mathbfr_t" class="level4">
<h4 class="anchored" data-anchor-id="a.-sampling-the-return-vector-mathbfr_t"><strong>A. Sampling the Return Vector</strong> <span class="math inline">\(\mathbf{r}_t\)</span></h4>
<p><strong>Conditional Distribution</strong>:</p>
<p><span class="math display">\[
P(\mathbf{r}_t \mid \boldsymbol{\mu}_t, b^{\mathbf{v}}_t, \mathbf{v}_t, \Sigma, P, \Omega) \propto P(\mathbf{r}_t \mid \boldsymbol{\mu}_t, \Sigma) \cdot P(\mathbf{v}_t \mid \mathbf{r}_t, b^{\mathbf{v}}_t, \Omega)
\]</span></p>
<p><strong>Derivation of Conditional Dependencies</strong>:</p>
<ol type="1">
<li><p><strong>Prior Distribution</strong>:</p>
<p>The prior for <span class="math inline">\(\mathbf{r}_t\)</span> given <span class="math inline">\(\boldsymbol{\mu}_t\)</span>:</p>
<p><span class="math display">\[
\mathbf{r}_t \mid \boldsymbol{\mu}_t \sim \mathcal{N}(\boldsymbol{\mu}_t, \Sigma)
\]</span></p></li>
<li><p><strong>Likelihood from Expert Views</strong>:</p>
<p>The likelihood from the expert view <span class="math inline">\(\mathbf{v}_t\)</span>:</p>
<p><span class="math display">\[
\mathbf{v}_t \mid \mathbf{r}_t, b^{\mathbf{v}}_t \sim \mathcal{N}(P\mathbf{r}_t + b^{\mathbf{v}}_t, \Omega)
\]</span></p></li>
<li><p><strong>Posterior Distribution</strong>:</p>
<p>Combining the prior and likelihood, the posterior is proportional to:</p>
<p><span class="math display">\[
P(\mathbf{r}_t \mid \cdots) \propto \exp\left( -\frac{1}{2} (\mathbf{r}_t - \boldsymbol{\mu}_t)^\top \Sigma^{-1} (\mathbf{r}_t - \boldsymbol{\mu}_t) - \frac{1}{2} (\mathbf{v}_t - P\mathbf{r}_t - b^{\mathbf{v}}_t)^\top \Omega^{-1} (\mathbf{v}_t - P\mathbf{r}_t - b^{\mathbf{v}}_t) \right)
\]</span></p>
<p>This is a Gaussian distribution, and by completing the square, we find:</p>
<ul>
<li><p><strong>Posterior Mean</strong>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_{\mathbf{r}_t} = \left( \Sigma^{-1} + P^\top \Omega^{-1} P \right)^{-1} \left( \Sigma^{-1} \boldsymbol{\mu}_t + P^\top \Omega^{-1} (\mathbf{v}_t - b^{\mathbf{v}}_t) \right)
\]</span></p></li>
<li><p><strong>Posterior Covariance</strong>:</p>
<p><span class="math display">\[
\Sigma_{\mathbf{r}_t} = \left( \Sigma^{-1} + P^\top \Omega^{-1} P \right)^{-1}
\]</span></p></li>
</ul></li>
</ol>
<p><strong>Sampling Step</strong>:</p>
<ul>
<li>Draw <span class="math inline">\(\mathbf{r}_t\)</span> from <span class="math inline">\(\mathcal{N}(\boldsymbol{\mu}_{\mathbf{r}_t}, \Sigma_{\mathbf{r}_t})\)</span>.</li>
</ul>
</section>
<section id="b.-sampling-the-return-bias-bboldsymbolmu" class="level4">
<h4 class="anchored" data-anchor-id="b.-sampling-the-return-bias-bboldsymbolmu"><strong>B. Sampling the Return Bias</strong> <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span></h4>
<p><strong>Conditional Distribution</strong>:</p>
<p><span class="math display">\[
P(b^{\boldsymbol{\mu}} \mid \{\boldsymbol{\mu}_s\}_{s=1}^t, \alpha, \beta, \delta_0^{\boldsymbol{\mu}}, \Theta_0^{\boldsymbol{\mu}}) \propto P(\{ \boldsymbol{\mu}_s \}_{s=1}^t \mid b^{\boldsymbol{\mu}}, \alpha, \beta) \cdot P(b^{\boldsymbol{\mu}} \mid \delta_0^{\boldsymbol{\mu}}, \Theta_0^{\boldsymbol{\mu}})
\]</span></p>
<p><strong>Derivation of Conditional Dependencies</strong>:</p>
<ol type="1">
<li><p><strong>Prior for</strong> <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span>:</p>
<p><span class="math display">\[
b^{\boldsymbol{\mu}} \sim \mathcal{N}(\delta_0^{\boldsymbol{\mu}}, \Theta_0^{\boldsymbol{\mu}})
\]</span></p></li>
<li><p><strong>Likelihood from Mean Vectors</strong>:</p>
<p>Each mean vector <span class="math inline">\(\boldsymbol{\mu}_s\)</span> depends on <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_s \mid b^{\boldsymbol{\mu}} \sim \mathcal{N}(\alpha + b^{\boldsymbol{\mu}}, \beta)
\]</span></p></li>
<li><p><strong>Posterior Distribution</strong>:</p>
<p>Since all <span class="math inline">\(\boldsymbol{\mu}_s\)</span> are conditionally independent given <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span>, the joint likelihood is:</p></li>
</ol>
<p><span class="math display">\[
P(\{ \boldsymbol{\mu}_s \}_{s=1}^t \mid b^{\boldsymbol{\mu}}) = \prod_{s=1}^t P(\boldsymbol{\mu}_s \mid b^{\boldsymbol{\mu}})
\]</span></p>
<p>The posterior is Gaussian with:</p>
<ul>
<li><strong>Posterior Covariance</strong>:</li>
</ul>
<p><span class="math display">\[
\Sigma_{b^{\boldsymbol{\mu}}} = \left( t \beta^{-1} + (\Theta_0^{\boldsymbol{\mu}})^{-1} \right)^{-1}
\]</span></p>
<ul>
<li><p><strong>Posterior Mean</strong>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_{b^{\boldsymbol{\mu}}} = \Sigma_{b^{\boldsymbol{\mu}}} \left( \beta^{-1} \sum_{s=1}^t (\boldsymbol{\mu}_s - \alpha) + (\Theta_0^{\boldsymbol{\mu}})^{-1} \delta_0^{\boldsymbol{\mu}} \right)
\]</span></p></li>
</ul>
<p><strong>Sampling Step</strong>:</p>
<ul>
<li>Draw <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span> from <span class="math inline">\(\mathcal{N}(\boldsymbol{\mu}_{b^{\boldsymbol{\mu}}}, \Sigma_{b^{\boldsymbol{\mu}}})\)</span>.</li>
</ul>
</section>
<section id="c.-sampling-the-mean-vectors-boldsymbolmu_s_s1t" class="level4">
<h4 class="anchored" data-anchor-id="c.-sampling-the-mean-vectors-boldsymbolmu_s_s1t"><strong>C. Sampling the Mean Vectors</strong> <span class="math inline">\(\{\boldsymbol{\mu}_s\}_{s=1}^t\)</span></h4>
<p><strong>Conditional Distribution</strong>:</p>
<p>For each <span class="math inline">\(s = 1, \dots, t\)</span>:</p>
<p><span class="math display">\[
P(\boldsymbol{\mu}_s \mid \mathbf{r}_s, b^{\boldsymbol{\mu}}, \alpha, \beta, \Sigma) \propto P(\mathbf{r}_s \mid \boldsymbol{\mu}_s, \Sigma) \cdot P(\boldsymbol{\mu}_s \mid b^{\boldsymbol{\mu}}, \alpha, \beta)
\]</span></p>
<p><strong>Derivation of Conditional Dependencies</strong>:</p>
<ol type="1">
<li><p><strong>Prior for</strong> <span class="math inline">\(\boldsymbol{\mu}_s\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_s \mid b^{\boldsymbol{\mu}} \sim \mathcal{N}(\alpha + b^{\boldsymbol{\mu}}, \beta)
\]</span></p></li>
<li><p><strong>Likelihood from Returns</strong>:</p>
<p><span class="math display">\[
\mathbf{r}_s \mid \boldsymbol{\mu}_s \sim \mathcal{N}(\boldsymbol{\mu}_s, \Sigma)
\]</span></p></li>
<li><p><strong>Posterior Distribution</strong>:</p>
<p>The posterior is Gaussian with:</p>
<ul>
<li><p><strong>Posterior Covariance</strong>:</p>
<p><span class="math display">\[
\Sigma_{\boldsymbol{\mu}_s} = \left( \Sigma^{-1} + \beta^{-1} \right)^{-1}
\]</span></p></li>
<li><p><strong>Posterior Mean</strong>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_{\boldsymbol{\mu}_s} = \Sigma_{\boldsymbol{\mu}_s} \left( \Sigma^{-1} \mathbf{r}_s + \beta^{-1} (\alpha + b^{\boldsymbol{\mu}}) \right)
\]</span></p></li>
</ul></li>
</ol>
<p><strong>Sampling Step</strong>:</p>
<ul>
<li>For each <span class="math inline">\(s\)</span>, draw <span class="math inline">\(\boldsymbol{\mu}_s\)</span> from <span class="math inline">\(\mathcal{N}(\boldsymbol{\mu}_{\boldsymbol{\mu}_s}, \Sigma_{\boldsymbol{\mu}_s})\)</span>.</li>
</ul>
</section>
<section id="d.-sampling-the-view-biases-bmathbfv_s_s1t" class="level4">
<h4 class="anchored" data-anchor-id="d.-sampling-the-view-biases-bmathbfv_s_s1t"><strong>D. Sampling the View Biases</strong> <span class="math inline">\(\{b^{\mathbf{v}}_s\}_{s=1}^t\)</span></h4>
<p><strong>Conditional Distribution</strong>:</p>
<p>For each <span class="math inline">\(s = 1, \dots, t\)</span>:</p>
<p><span class="math display">\[
P(b^{\mathbf{v}}_s \mid \mathbf{v}_s, \mathbf{r}_s, \delta^{\mathbf{v}}, \Theta^{\mathbf{v}}, \Omega, P) \propto P(\mathbf{v}_s \mid \mathbf{r}_s, b^{\mathbf{v}}_s, \Omega) \cdot P(b^{\mathbf{v}}_s \mid \delta^{\mathbf{v}}, \Theta^{\mathbf{v}})
\]</span></p>
<p><strong>Derivation of Conditional Dependencies</strong>:</p>
<ol type="1">
<li><p><strong>Prior for</strong> <span class="math inline">\(b^{\mathbf{v}}_s\)</span>:</p>
<p><span class="math display">\[
b^{\mathbf{v}}_s \sim \mathcal{N}(\delta^{\mathbf{v}}, \Theta^{\mathbf{v}})
\]</span></p></li>
<li><p><strong>Likelihood from Expert Views</strong>:</p>
<p><span class="math display">\[
\mathbf{v}_s \mid \mathbf{r}_s, b^{\mathbf{v}}_s \sim \mathcal{N}(P\mathbf{r}_s + b^{\mathbf{v}}_s, \Omega)
\]</span></p></li>
<li><p><strong>Posterior Distribution</strong>:</p></li>
</ol>
<p>The posterior is Gaussian with:</p>
<ul>
<li><strong>Posterior Covariance</strong>:</li>
</ul>
<p><span class="math display">\[
\Sigma_{b^{\mathbf{v}}_s} = \left( \Omega^{-1} + (\Theta^{\mathbf{v}})^{-1} \right)^{-1}
\]</span></p>
<ul>
<li><p><strong>Posterior Mean</strong>:</p>
<p><span class="math display">\[
\boldsymbol{\mu}_{b^{\mathbf{v}}_s} = \Sigma_{b^{\mathbf{v}}_s} \left( \Omega^{-1} (\mathbf{v}_s - P\mathbf{r}_s) + (\Theta^{\mathbf{v}})^{-1} \delta^{\mathbf{v}} \right)
\]</span></p></li>
</ul>
<p><strong>Sampling Step</strong>:</p>
<ul>
<li>For each <span class="math inline">\(s\)</span>, draw <span class="math inline">\(b^{\mathbf{v}}_s\)</span> from <span class="math inline">\(\mathcal{N}(\boldsymbol{\mu}_{b^{\mathbf{v}}_s}, \Sigma_{b^{\mathbf{v}}_s})\)</span>.</li>
</ul>
</section>
<section id="e.-sampling-the-parameters-for-view-bias-deltamathbfv-and-thetamathbfv" class="level4">
<h4 class="anchored" data-anchor-id="e.-sampling-the-parameters-for-view-bias-deltamathbfv-and-thetamathbfv"><strong>E. Sampling the Parameters for View Bias</strong> <span class="math inline">\(\delta^{\mathbf{v}}\)</span> and <span class="math inline">\(\Theta^{\mathbf{v}}\)</span></h4>
<p><strong>Conditional Distribution</strong>:</p>
<p><span class="math display">\[
P(\delta^{\mathbf{v}}, \Theta^{\mathbf{v}} \mid \{b^{\mathbf{v}}_s\}_{s=1}^t, \eta_0^{\mathbf{v}}, \kappa_0^{\mathbf{v}}, \nu_0^{\mathbf{v}}, \Lambda_0^{\mathbf{v}}) \propto \left[ \prod_{s=1}^t P(b^{\mathbf{v}}_s \mid \delta^{\mathbf{v}}, \Theta^{\mathbf{v}}) \right] \cdot P(\delta^{\mathbf{v}}, \Theta^{\mathbf{v}})
\]</span></p>
<p><strong>Derivation of Conditional Dependencies</strong>:</p>
<ol type="1">
<li><p><strong>Prior for</strong> <span class="math inline">\((\delta^{\mathbf{v}}, \Theta^{\mathbf{v}})\)</span>:</p>
<p><span class="math display">\[
(\delta^{\mathbf{v}}, \Theta^{\mathbf{v}}) \sim \text{Normal-Inverse-Wishart}(\eta_0^{\mathbf{v}}, \kappa_0^{\mathbf{v}}, \nu_0^{\mathbf{v}}, \Lambda_0^{\mathbf{v}})
\]</span></p></li>
<li><p><strong>Likelihood from View Biases</strong>:</p>
<p>Each <span class="math inline">\(b^{\mathbf{v}}_s\)</span> depends on <span class="math inline">\((\delta^{\mathbf{v}}, \Theta^{\mathbf{v}})\)</span>:</p>
<p><span class="math display">\[
b^{\mathbf{v}}_s \mid \delta^{\mathbf{v}}, \Theta^{\mathbf{v}} \sim \mathcal{N}(\delta^{\mathbf{v}}, \Theta^{\mathbf{v}})
\]</span></p></li>
<li><p><strong>Posterior Distribution</strong>:</p>
<p>The Normal-Inverse-Wishart distribution is conjugate to the Gaussian likelihood. The posterior parameters are updated as:</p>
<ul>
<li><p><strong>Updated Parameters</strong>:</p>
<p><span class="math display">\[
\kappa_n = \kappa_0^{\mathbf{v}} + t
\]</span></p>
<p><span class="math display">\[
\eta_n = \frac{\kappa_0^{\mathbf{v}} \eta_0^{\mathbf{v}} + t \bar{b}^{\mathbf{v}}}{\kappa_0^{\mathbf{v}} + t}
\]</span></p>
<p><span class="math display">\[
\nu_n = \nu_0^{\mathbf{v}} + t
\]</span></p>
<p><span class="math display">\[
\Lambda_n = \Lambda_0^{\mathbf{v}} + S + \frac{\kappa_0^{\mathbf{v}} t}{\kappa_0^{\mathbf{v}} + t} (\bar{b}^{\mathbf{v}} - \eta_0^{\mathbf{v}})(\bar{b}^{\mathbf{v}} - \eta_0^{\mathbf{v}})^\top
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\bar{b}^{\mathbf{v}} = \frac{1}{t} \sum_{s=1}^t b^{\mathbf{v}}_s\)</span></li>
<li><span class="math inline">\(S = \sum_{s=1}^t (b^{\mathbf{v}}_s - \bar{b}^{\mathbf{v}})(b^{\mathbf{v}}_s - \bar{b}^{\mathbf{v}})^\top\)</span></li>
</ul></li>
</ul></li>
</ol>
<p><strong>Sampling Step</strong>:</p>
<ul>
<li>Sample <span class="math inline">\(\Theta^{\mathbf{v}}\)</span> from an Inverse-Wishart distribution <span class="math inline">\(\mathcal{W}^{-1}(\Lambda_n, \nu_n)\)</span>.</li>
<li>Given <span class="math inline">\(\Theta^{\mathbf{v}}\)</span>, sample <span class="math inline">\(\delta^{\mathbf{v}}\)</span> from <span class="math inline">\(\mathcal{N}(\eta_n, \Theta^{\mathbf{v}} / \kappa_n)\)</span>.</li>
</ul>
</section>
</div>
</div>
</div>
<p>Overall, the Gibbs sampling procedure relies on the conditional dependencies among the parameters:</p>
<ul>
<li><span class="math inline">\(\mathbf{r}_t\)</span> depends on <span class="math inline">\(\boldsymbol{\mu}_t\)</span> and <span class="math inline">\(b^{\mathbf{v}}_t\)</span>.</li>
<li><span class="math inline">\(b^{\boldsymbol{\mu}}\)</span> depends on all <span class="math inline">\(\boldsymbol{\mu}_s\)</span>.</li>
<li>Each <span class="math inline">\(\boldsymbol{\mu}_s\)</span> depends on <span class="math inline">\(\mathbf{r}_s\)</span> and <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span>.</li>
<li>Each <span class="math inline">\(b^{\mathbf{v}}_s\)</span> depends on <span class="math inline">\(\mathbf{v}_s\)</span>, <span class="math inline">\(\mathbf{r}_s\)</span>, <span class="math inline">\(\delta^{\mathbf{v}}\)</span>, and <span class="math inline">\(\Theta^{\mathbf{v}}\)</span>.</li>
<li><span class="math inline">\(\delta^{\mathbf{v}}\)</span> and <span class="math inline">\(\Theta^{\mathbf{v}}\)</span> depend on all <span class="math inline">\(b^{\mathbf{v}}_s\)</span>.</li>
</ul>
<p>These dependencies ensure that information flows among the parameters during the sampling process, allowing for joint estimation.</p>
</section>
<section id="gibbs-sampling-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="gibbs-sampling-algorithm">Gibbs Sampling Algorithm</h3>
<p>The Gibbs sampling algorithm proceeds as follows:</p>
<ol type="1">
<li><p><strong>Initialization</strong>: Start with initial values for all unknown parameters.</p></li>
<li><p><strong>Iterative Sampling</strong>:</p>
<p>For each iteration:</p>
<ul>
<li><strong>Sample</strong> <span class="math inline">\(\mathbf{r}_t\)</span> given current <span class="math inline">\(\boldsymbol{\mu}_t\)</span> and <span class="math inline">\(b^{\mathbf{v}}_t\)</span>.</li>
<li><strong>Sample</strong> <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span> given current <span class="math inline">\(\{\boldsymbol{\mu}_s\}_{s=1}^t\)</span>.</li>
<li><strong>Sample</strong> <span class="math inline">\(\{\boldsymbol{\mu}_s\}_{s=1}^t\)</span> given current <span class="math inline">\(\{\mathbf{r}_s\}_{s=1}^t\)</span> and <span class="math inline">\(b^{\boldsymbol{\mu}}\)</span>.</li>
<li><strong>Sample</strong> <span class="math inline">\(\{b^{\mathbf{v}}_s\}_{s=1}^t\)</span> given current <span class="math inline">\(\{\mathbf{v}_s\}_{s=1}^t\)</span>, <span class="math inline">\(\{\mathbf{r}_s\}_{s=1}^t\)</span>, <span class="math inline">\(\delta^{\mathbf{v}}\)</span>, and <span class="math inline">\(\Theta^{\mathbf{v}}\)</span>.</li>
<li><strong>Sample</strong> <span class="math inline">\(\delta^{\mathbf{v}}\)</span> and <span class="math inline">\(\Theta^{\mathbf{v}}\)</span> given current <span class="math inline">\(\{b^{\mathbf{v}}_s\}_{s=1}^t\)</span>.</li>
</ul></li>
<li><p><strong>Convergence and Inference</strong>:</p>
<ul>
<li>After sufficient iterations, the samples approximate the joint posterior distribution.</li>
<li>Use the samples to estimate the posterior mean and covariance of <span class="math inline">\(\mathbf{r}_t\)</span> for portfolio optimization.</li>
</ul></li>
</ol>
</section>
</section>
<section id="robust-portfolio-optimization" class="level2">
<h2 class="anchored" data-anchor-id="robust-portfolio-optimization">Robust Portfolio Optimization</h2>
<p>Let us look at the following risk-adjusted reward function to maximize.</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{max}} \quad \mathbf{w}^T \boldsymbol{\mu} - \lambda \mathbf{w}^T \Sigma \mathbf{w}
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a user-defined risk aversion parameter, and the input parameters <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\Sigma\)</span> represent the expected future asset returns and the covariance matrix, respectively. In reality, these parameters <span class="math inline">\(\boldsymbol{\theta}=(\boldsymbol{\mu}, \Sigma)\)</span> need to be estimated, using linear factor models or nonlinear machine learning models, which leads to the following formulation.</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{max}} \quad \mathbf{w}^T \hat{\boldsymbol{\mu}} - \lambda \mathbf{w}^T \hat{\Sigma} \mathbf{w}
\]</span></p>
<p>These estimates <span class="math inline">\(\hat{\boldsymbol{\mu}}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span> are unlikely to be completely accurate given the nature of the financial data, resulting in an estimation error. The resulting optimal portfolio (specifically, the allocation weights <span class="math inline">\(\mathbf{w}^*(\boldsymbol{\hat \theta})\)</span>) was shown to be sensitive to these estimates and may likely differ from the optimal solution <span class="math inline">\(\mathbf{w}^*(\boldsymbol{\theta})\)</span>. If we have good estimates such that <span class="math inline">\(\boldsymbol{\hat \theta} \approx \boldsymbol{\theta}\)</span>, then we can expect <span class="math inline">\(\mathbf{w}^*(\boldsymbol{\hat \theta}) \approx \mathbf{w}^*(\boldsymbol{ \theta})\)</span>. However, these parameters are extremely difficult to predict, given the dynamic nature of the financial market. In fact, it is ten times more sensitive to predict <span class="math inline">\(\hat{\boldsymbol{\mu}}\)</span> than to <span class="math inline">\(\hat{\Sigma}\)</span>. This means that a small change in <span class="math inline">\(\mathbf{\hat{\mu}}\)</span> results in a large change in the resulting allocation <span class="math inline">\(\mathbf{\hat w^*}\)</span>.</p>
<p>In other words, a small change in the input parameter causes a large change in the output decision, making the portfolio choice sensitive. Applying robust optimization is one way to make the portfolio less sensitive to the estimated input parameters of the predicted <span class="math inline">\(\hat{\boldsymbol{\mu}}\)</span> and <span class="math inline">\(\hat{\Sigma}\)</span>.</p>
<p>There are three common approaches to robust optimization, including stochastic robust optimization, which involves expectations; worst-case robust optimization, which optimizes the worst-case scenarios; and chance robust optimization, which optimizes the worst-case scenarios up to a predefined probability. In the following, we focus on the robust optimization in the worst-case.</p>
<p>The following three types of uncertainty region for parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> against the estimated or nominal parameter vector <span class="math inline">\(\boldsymbol{\hat \theta}\)</span> are commonly used:</p>
<ul>
<li><p><strong>Box region:</strong></p>
<p><span class="math display">\[
U_{\boldsymbol{\theta}}=\{\boldsymbol{\theta} \mid \|\boldsymbol{\theta} - \boldsymbol{\hat \theta}\|_{\infty} \leq \delta\}
\]</span></p></li>
<li><p><strong>Sphere region:</strong></p>
<p><span class="math display">\[
U_{\boldsymbol{\theta}}=\{\boldsymbol{\theta} \mid \|\boldsymbol{\theta} - \boldsymbol{\hat \theta}\|_{2} \leq \delta\}
\]</span></p></li>
<li><p><strong>Elliptical region:</strong></p>
<p><span class="math display">\[
U_{\boldsymbol{\theta}}=\{\boldsymbol{\theta} \mid (\boldsymbol{\theta} - \boldsymbol{\hat \theta})^T \boldsymbol{S}^{-1} (\boldsymbol{\theta} - \boldsymbol{\hat \theta}) \leq \delta^2\}
\]</span></p></li>
</ul>
<p>where <span class="math inline">\(\boldsymbol{S}\)</span> is a symmetric and positive definite matrix that defines the shape of the ellipsoid. The ellipsoid axes are aligned with the eigenvectors of <span class="math inline">\(\boldsymbol{S}\)</span>, and the lengths of the axes are proportional to the square root of the eigenvalues of <span class="math inline">\(\boldsymbol{S}\)</span>. Unlike the box or sphere, which represent uncertainties in terms of absolute deviations and Euclidean distances, respectively, the elliptical set accounts for correlations and relative scaling between the parameters. If <span class="math inline">\(\boldsymbol{S}\)</span> is the identity matrix, the ellipsoid becomes a sphere.</p>
<p>Assume that these two random parameters will change within the corresponding uncertainty set <span class="math inline">\(U_{\boldsymbol{\mu}}\)</span> and <span class="math inline">\(U_\Sigma\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\mu} \in U_\boldsymbol{\mu}
\]</span></p>
<p><span class="math display">\[
\Sigma \in U_\Sigma
\]</span></p>
<p>We will consider the worst-case scenarios within these uncertainty sets, giving rise to the following robust MVO formulation:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \underset{\boldsymbol{\mu} \in U_\boldsymbol{\mu}}{\text{min}} \boldsymbol{\mu}^T \mathbf{w} - \lambda \underset{\Sigma \in U_\Sigma}{\text{max}} \mathbf{w}^T \Sigma \mathbf{w} \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; &amp; &amp; w_i \geq 0 \, \quad \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]</span></p>
<section id="box-uncertainty-over-mean-return" class="level3">
<h3 class="anchored" data-anchor-id="box-uncertainty-over-mean-return">Box Uncertainty over Mean Return</h3>
<p>Now, let us consider a box uncertainty set on the mean return vector. Assume a fixed covariance matrix <span class="math inline">\(\Sigma\)</span> and consider the following set of component-wise box uncertainty <span class="math inline">\(U_{\boldsymbol{\mu}}\)</span> for <span class="math inline">\(\boldsymbol{\mu}\)</span>:</p>
<p><span class="math display">\[
U_\boldsymbol{\mu} = \{\boldsymbol{\mu} \mid -\mathbf{\delta} \leq \boldsymbol{\mu} - \bar{\boldsymbol{\mu}} \leq \mathbf{\delta} \} \\
= \{\boldsymbol{\mu} \mid  \|\boldsymbol{\mu} - \bar{\boldsymbol{\mu}}\|_{\infty} \leq \mathbf{\delta} \}
\]</span></p>
<p>where <span class="math inline">\(\bar{\boldsymbol{\mu}}\)</span> is a predefined location of the box uncertainty set and is used as our parameter estimate <span class="math inline">\(\hat{\boldsymbol{\mu}} = \bar{\boldsymbol{\mu}}\)</span>, <span class="math inline">\(\mathbf{\delta}\)</span> is a predefined size of the box uncertainty set, and <span class="math inline">\(\|\boldsymbol{\mu} - \bar{\boldsymbol{\mu}}\|_{\infty}\)</span> denotes the infinite norm.</p>
<p>The MVO formulation now becomes:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \underset{\boldsymbol{\mu} \in U_\boldsymbol{\mu}}{\text{min}} \boldsymbol{\mu}^T \mathbf{w} - \lambda \mathbf{w}^T  \hat{\Sigma} \mathbf{w} \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1
\end{aligned}
\]</span></p>
<p>where we now consider a more general case after removing the nonnegative constraint.</p>
<p>We note an equivalent form of the inner minimization problem.</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{max}}
\underset{\boldsymbol{\mu} \in U_\boldsymbol{\mu}}{\text{min}} \boldsymbol{\mu}^T \mathbf{w} - \lambda \mathbf{w}^T  \hat{\Sigma} \mathbf{w}  = \underset{\mathbf{w}}{\text{max}}
\bar{\boldsymbol{\mu}}^T \mathbf{w} - \mathbf{\delta} \|\mathbf{w}\|_1 - \lambda \mathbf{w}^T  \hat{\Sigma} \mathbf{w}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The robust portfolio optimization problem with box uncertainty set in the estimated mean return vector is equivalent to adding an <span class="math inline">\(l_1\)</span> norm penalty to the objective function:</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{max}}
\underset{\boldsymbol{\mu} \in U_\boldsymbol{\mu}}{\text{min}} \boldsymbol{\mu}^T \mathbf{w} - \lambda \mathbf{w}^T  \hat{\Sigma} \mathbf{w}  = \underset{\mathbf{w}}{\text{max}}
\bar{\boldsymbol{\mu}}^T \mathbf{w} - \mathbf{\delta} \|\mathbf{w}\|_1 - \lambda \mathbf{w}^T  \hat{\Sigma} \mathbf{w}
\]</span></p>
<p><strong>Proof</strong></p>
<p>We want to show that minimizing the expected return results in the worst-case return across the entire domain <span class="math inline">\(U_{\boldsymbol{\mu}}\)</span>. Since <span class="math inline">\(\bar{\boldsymbol{\mu}} - \mathbf{\delta} \leq \boldsymbol{\mu} \leq \bar{\boldsymbol{\mu}} + \mathbf{\delta}\)</span>, and the function <span class="math inline">\(\boldsymbol{\mu}^T \mathbf{w}\)</span> is monotone in <span class="math inline">\(\mu\)</span>, we observe that the minimal value is obtained at the endpoints, depending on the sign of the weight value <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p>Specifically, for any asset weight <span class="math inline">\(w_i\)</span>, <span class="math inline">\(i \in \{1,\dots,n\}\)</span></p>
<ul>
<li>when <span class="math inline">\(w_i \geq 0\)</span>, <span class="math inline">\(\underset{\mu_i \in U_{\mu_i}}{\text{min}} \mu_i w_i = \bar{\mu_i}w_i - \delta w_i\)</span>.</li>
<li>when <span class="math inline">\(w_i &lt; 0\)</span>, <span class="math inline">\(\underset{\mu_i \in U_{\mu_i}}{\text{min}} \mu_i w_i = \bar{\mu_i}w_i + \delta w_i\)</span>.</li>
</ul>
<p>Both cases can be written together via the following:</p>
<p><span class="math display">\[
\underset{\mu_i \in U_{\mu_i}}{\text{min}} \mu_i w_i = \bar{\mu_i}w_i - \delta |w_i|
\]</span></p>
<p>Thus, we can convert the previous problem to the following equivalent formulation.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \bar{\boldsymbol{\mu}}^T \mathbf{w} - \mathbf{\delta} \|\mathbf{w}\|_1 - \lambda \mathbf{w}^T  \hat{\Sigma} \mathbf{w} \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1
\end{aligned}
\]</span></p>
<p>where adding a set of box uncertainties to the optimization problem is equivalent to adding an <span class="math inline">\(L_1\)</span> norm penalty to the objective function.</p>
<p>We note that the original equivalence in the general context was first shown in <span class="citation" data-cites="sra2012optimization">Sra, Nowozin, and Wright (<a href="#ref-sra2012optimization" role="doc-biblioref">2012</a>)</span>, and we extend it to the specific MVO setting here. As shown in the following section, this equivalence also applies to the box uncertainty set over the covariance matrix estimate.</p>
</div>
</div>
</div>
<p>To solve this, we resort to a standard trick to remove the absolute value operation in <span class="math inline">\(\|\mathbf{w}\|_1\)</span>, giving:</p>
<p><span class="math display">\[
\begin{align}
\underset{\mathbf{w}, \mathbf{\psi}}{\text{maximize}} \quad &amp; \bar{\boldsymbol{\mu}}^T \mathbf{w} - \mathbf{\delta} \mathbf{\psi} - \lambda \mathbf{w}^T  \hat{\Sigma} \mathbf{w} \\
\text{subject to} \quad &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; \psi_i \geq w_i, \quad \psi_i \geq -w_i, \quad i \in \{1,\dots,n\}
\end{align}
\]</span></p>
<p>Here, we introduce a new unknown vector <span class="math inline">\(\mathbf{\psi}\)</span> to be optimized together with <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p>It should be noted that the size parameter <span class="math inline">\(\delta\)</span> for the uncertainty set <span class="math inline">\(U_{\mu}\)</span> is often arbitrarily selected or based on a certain operational constraint or investor preference. One way is to learn the optimal <span class="math inline">\(\delta\)</span> based on the previous Bayesian graphical model when predicting stock returns. <span class="math inline">\(\delta\)</span> can be determined by the prediction uncertainty parameter learned using the Bayesian network.</p>
</section>
<section id="box-uncertainty-on-the-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="box-uncertainty-on-the-covariance-matrix">Box Uncertainty on the Covariance Matrix</h3>
<p>In a manner analogous to the box uncertainty applied to the mean return vector, a similar concept can be implemented for the covariance matrix of the portfolio. Let us denote the nominal covariance matrix by <span class="math inline">\(\bar{\Sigma}\)</span>, typically estimated using historical sample data. Rather than using a fixed <span class="math inline">\(\bar{\Sigma}\)</span> in MVO and considering the uncertainty in the covariance matrix, we introduce a degree of variability by incorporating an additive random perturbation matrix <span class="math inline">\(\Delta\)</span>. This matrix is a symmetric <span class="math inline">\(N \times N\)</span> matrix and adheres to the following inequality constraint:</p>
<p><span class="math display">\[
|\Delta_{ij}| \leq \kappa (\bar{\Sigma}_{ii} \bar{\Sigma}_{jj})^{\frac{1}{2}}    
\]</span></p>
<p>Here, each element <span class="math inline">\(\Delta_{ij}\)</span> is bounded by a scaled version of the square root of <span class="math inline">\(\sigma_i^2 \sigma_j^2\)</span>, for each pair of assets <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. This implies that the allowed deviation for the covariance estimate between assets <em>i</em> and <em>j</em> is proportionate to the geometric mean of their individual variances. Thus, larger variances (indicating higher volatility) can lead to greater uncertainty in the covariance estimates.</p>
<p>The scaling factor <span class="math inline">\(\kappa\)</span> typically takes values such as 0.02 or 0.05, indicating moderate levels of uncertainty. This factor reflects the proportional uncertainty in the covariance estimate, acknowledging potential estimation errors or fluctuations over time. Consequently, the covariance matrix <span class="math inline">\(\Sigma\)</span> used in the optimization model is represented as</p>
<p><span class="math display">\[
\Sigma = \bar{\Sigma} + \Delta
\]</span></p>
<p>In this model, the constrained <span class="math inline">\(\Delta\)</span> imposes a percentage-based relative threshold. The input parameter <span class="math inline">\(\Sigma\)</span> therefore belongs to the following uncertainty set:</p>
<p><span class="math display">\[
U_{\Sigma} = \left\{\Sigma \mid \bar{\Sigma}_{ij} - \kappa (\bar{\Sigma}_{ii} \bar{\Sigma}_{jj})^{\frac{1}{2}} \leq \Sigma_{ij} \leq \bar{\Sigma}_{ij} + \kappa (\bar{\Sigma}_{ii} \bar{\Sigma}_{jj})^{\frac{1}{2}}, \forall i, j \in \{1,\dots,N\}\right\}
\]</span></p>
<p>Or equivalently,</p>
<p><span class="math display">\[
U_{\Delta} = \left\{ \Delta \mid |\Delta_{ij}| \leq \kappa  \sqrt{\bar{\Sigma}_{ii} \bar{\Sigma}_{jj}}, \forall i, j \right\}
\]</span></p>
<p>This contrasts with the absolute value-based threshold used for the mean return vector, and it aligns with the nature of covariance estimates which are often variable and subject to estimation errors. This approach to robust portfolio optimization helps to account for the uncertainty inherent in the estimation of covariances, leading to a portfolio construction that is potentially more resilient to estimation errors.</p>
<p>The robust MVO formulation now becomes:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \boldsymbol{\mu}^T \mathbf{w} - \lambda \underset{\Sigma \in U_{\Sigma}}{\text{max}} \mathbf{w}^T \Sigma \mathbf{w} \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; &amp; &amp; w_i \geq 0 \, \quad \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]</span></p>
<p>We can show that the robust MVO formulation is equivalent to adding a squared <span class="math inline">\(l_1\)</span> norm to the objective function.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \boldsymbol{\mu}^T \mathbf{w} - \lambda \mathbf{w}^T \bar{\Sigma} \mathbf{w} - \kappa \left( \sum_{i}  |w_i| (\bar{\Sigma}_{ii})^{\frac{1}{2}} \right)^2 \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; &amp; &amp; w_i \geq 0 \, \quad \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The robust portfolio optimization problem with element-wise box uncertainty set in the covariance matrix is equivalent to adding a squared <span class="math inline">\(l_1\)</span> norm penalty to the objective function:</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{max}}
\boldsymbol{\mu}^T \mathbf{w} - \lambda \underset{\Sigma \in U_{\Sigma}}{\text{max}} \mathbf{w}^T \Sigma \mathbf{w} =
\underset{\mathbf{w}}{\text{max}} \boldsymbol{\mu}^T \mathbf{w} - \lambda \mathbf{w}^T \bar{\Sigma} \mathbf{w} - \kappa \left( \sum_{i}  |w_i| (\bar{\Sigma}_{ii})^{\frac{1}{2}} \right)^2
\]</span></p>
<p><strong>Proof</strong></p>
<p>Let us focus on the inner maximization term <span class="math inline">\(\underset{\Sigma \in U_{\Sigma}}{\text{max}} \mathbf{w}^T \Sigma \mathbf{w}\)</span>, which can be further simplified as follows.</p>
<p><span class="math display">\[
\begin{aligned}
\underset{|\Delta_{ij}| \leq \kappa (\bar{\Sigma}_{ii} \bar{\Sigma}_{jj})^{\frac{1}{2}}}{\text{max}} \mathbf{w}^T \Sigma \mathbf{w}  &amp;= \underset{|\Delta_{ij}| \leq \kappa (\bar{\Sigma}_{ii} \bar{\Sigma}_{jj})^{\frac{1}{2}}}{\text{max}}  \mathbf{w}^T (\bar{\Sigma} + \Delta) \mathbf{w} \\
&amp;= \mathbf{w}^T \bar{\Sigma} \mathbf{w} + \underset{|\Delta_{ij}| \leq \kappa (\bar{\Sigma}_{ii} \bar{\Sigma}_{jj})^{\frac{1}{2}}}{\text{max}}  \mathbf{w}^T \Delta \mathbf{w} \\
&amp;= \mathbf{w}^T \bar{\Sigma} \mathbf{w} + \underset{|\Delta_{ij}| \leq \kappa (\bar{\Sigma}_{ii} \bar{\Sigma}_{jj})^{\frac{1}{2}}}{\text{max}} \sum_{i,j}  w_i w_j \Delta_{ij} \\
&amp;= \mathbf{w}^T \bar{\Sigma} \mathbf{w} + \kappa \sum_{i,j}  |w_i w_j| (\bar{\Sigma}_{ii} \bar{\Sigma}_{jj})^{\frac{1}{2}} \\
&amp;= \mathbf{w}^T \bar{\Sigma} \mathbf{w} + \kappa \left( \sum_{i}  |w_i| (\bar{\Sigma}_{ii})^{\frac{1}{2}} \right)^2
\end{aligned}
\]</span></p>
<p>where the third equality reflects the pairwise contributions of the uncertainty in covariance estimates, and the fourth equality represents the maximum additional variance due to the uncertainty in the covariance matrix. To see the fifth equality, note that the term <span class="math inline">\(\sum_{i,j}  |w_i w_j| (\bar{\Sigma}_{ii} \bar{\Sigma}_{jj})^{\frac{1}{2}}\)</span> mimics the portfolio variance calculated by weight <span class="math inline">\(|w_i|\)</span> for each asset <span class="math inline">\(i \in \{1,\dots,N\}\)</span> and a covariance matrix with perfect correlation, that is, <span class="math inline">\(\rho_{ij}=1\)</span> for all <span class="math inline">\(i,j \in \{1,\dots,N\}\)</span>.</p>
<p>Note that the additional term <span class="math inline">\(\kappa \left( \sum_{i}  |w_i| (\bar{\Sigma}_{ii})^{\frac{1}{2}} \right)^2\)</span> essentially adds a squared <span class="math inline">\(L_1\)</span> norm to the original objective function. This line of reasoning is similar to stochastic optimization, where a random parameter perturbed by Gaussian noise converts to an additional regularizer term to the original objective. The difference is that, instead of characterizing the statistical property of the random parameter in stochastic optimization, the worst-case optimization assumes that the true parameter lies in an uncertainty region centered around the estimated value.</p>
<p>The final robust MVO formulation becomes</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \boldsymbol{\mu}^T \mathbf{w} - \lambda \mathbf{w}^T \bar{\Sigma} \mathbf{w} - \kappa \left( \sum_{i}  |w_i| (\bar{\Sigma}_{ii})^{\frac{1}{2}} \right)^2 \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; &amp; &amp; w_i \geq 0 \, \quad \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="elliptical-uncertainty-over-mean-return" class="level3">
<h3 class="anchored" data-anchor-id="elliptical-uncertainty-over-mean-return">Elliptical Uncertainty Over Mean Return</h3>
<p>Setting a box constraint to the mean return is often too conservative, as the robust counterpart corresponds to a worst-case penalty in the objective function. Instead, we can introduce an elliptical uncertainty set that models the potential deviations of the expected returns from their nominal estimates <span class="math inline">\(\boldsymbol{\hat \mu}\)</span>. The rationale is to capture the potential deviations of the expected returns from their nominal estimates by considering the uncertainty in a multi-dimensional sense, which reflects the possible correlation in estimation errors of asset returns.</p>
<p>The elliptical uncertainty set is formally defined as:</p>
<p><span class="math display">\[
U_{\boldsymbol{\mu}} = \left\{ \boldsymbol{\mu} \mid (\boldsymbol{\mu} - \boldsymbol{\hat \mu})^T \boldsymbol{S}^{-1} (\boldsymbol{\mu} - \boldsymbol{\hat \mu}) \leq \delta^2 \right\}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{S}^{-1}\)</span> is the inverse of a positive definite matrix that shapes the ellipsoid, and <span class="math inline">\(\delta\)</span> represents the “radius” of the ellipsoid in standard deviations, delineating the extent to which <span class="math inline">\(\boldsymbol{\mu}\)</span> can differ from <span class="math inline">\(\boldsymbol{\hat \mu}\)</span>.</p>
<p>An equivalent representation of the uncertainty set, emphasizing its geometric nature, is as follows:</p>
<p><span class="math display">\[
U_{\boldsymbol{\mu}} = \left\{ \boldsymbol{\mu} = \boldsymbol{\hat \mu} + \delta | \boldsymbol{S}^{1/2} \mathbf{u} \mid \quad \| \mathbf{u}\|_2 \leq 1 \right\}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{u}\)</span> extends the surface of a unit ball, scaled by <span class="math inline">\(\delta\)</span> (a positive scalar) and transformed by the matrix <span class="math inline">\(\boldsymbol{S}^{1/2}\)</span>. The matrix <span class="math inline">\(\boldsymbol{S}\)</span>, which can be set as the sample-based covariance matrix <span class="math inline">\(\boldsymbol{\hat \Sigma}\)</span>, incorporates historical variances and covariances of asset returns, thus integrating our confidence in these estimates into the uncertainty set.</p>
<p>When applying this uncertainty set to the MVO problem, we arrive at a robust optimization formulation.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \underset{\boldsymbol{\mu} \in U_{\boldsymbol{\mu}}}{\text{min}} \quad \mathbf{w}^T \boldsymbol{\mu} - \lambda \mathbf{w}^T \hat{\Sigma} \mathbf{w} \\
&amp; \text{subject to}
&amp; &amp; \mathbf{1}^T \mathbf{w} = 1 \\
&amp; &amp; &amp; \mathbf{w} \geq \mathbf{0}
\end{aligned}
\]</span></p>
<p>We can show that the robust MVO formulation is equivalent to adding a squared <span class="math inline">\(l_2\)</span> norm to the objective function.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \mathbf{w}^T \boldsymbol{\hat \mu} - \lambda \mathbf{w}^T \boldsymbol{\hat \Sigma} \mathbf{w} - \delta \|(\boldsymbol{S}^{1/2})^T \mathbf{w}\|_2 \\
&amp; \text{subject to}
&amp; &amp; \mathbf{1}^T \mathbf{w} = 1 \\
&amp; &amp; &amp; \mathbf{w} \geq \mathbf{0}
\end{aligned}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Proposition</strong></p>
<p>The robust portfolio optimization problem with an elliptical uncertainty set over the mean return vector is equivalent to adding an <span class="math inline">\(l_2\)</span> norm penalty to the objective function:</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{max}}
\underset{\boldsymbol{\mu} \in U_{\boldsymbol{\mu}}}{\text{min}} \quad \mathbf{w}^T \boldsymbol{\mu} - \lambda \mathbf{w}^T \hat{\Sigma} \mathbf{w} = \underset{\mathbf{w}}{\text{max}}
\mathbf{w}^T \boldsymbol{\hat \mu} - \lambda \mathbf{w}^T \boldsymbol{\hat \Sigma} \mathbf{w} - \delta \|(\boldsymbol{S}^{1/2})^T \mathbf{w}\|_2
\]</span></p>
<p><strong>Proof</strong></p>
<p>The inner minimization problem can be expressed as</p>
<p><span class="math display">\[
\begin{aligned}
\underset{\boldsymbol{\mu} \in U_{\boldsymbol{\mu}}}{\text{min}} \quad \mathbf{w}^T \boldsymbol{\mu}
&amp;= \underset{\|\mathbf{u}\|_2 \leq 1}{\text{min}} \quad \mathbf{w}^T (\boldsymbol{\hat \mu} + \delta \boldsymbol{S}^{1/2} \mathbf{u}) \\
&amp;= \mathbf{w}^T \boldsymbol{\hat \mu} + \underset{\|\mathbf{u}\|_2 \leq 1}{\text{min}} \quad \delta \mathbf{u}^T (\boldsymbol{S}^{1/2})^T \mathbf{w} \\
&amp;= \mathbf{w}^T \boldsymbol{\hat \mu} - \delta \|(\boldsymbol{S}^{1/2})^T \mathbf{w}\|_2
\end{aligned}
\]</span></p>
<p>This simplification leverages the property that the minimum value of <span class="math inline">\(\mathbf{u}^T \mathbf{z}\)</span> for a given vector <span class="math inline">\(\mathbf{z}\)</span> and <span class="math inline">\(\|\mathbf{u}\|_2 \leq 1\)</span> is <span class="math inline">\(-\|\mathbf{z}\|_2\)</span>.</p>
<p>Consequently, the robust MVO problem is formulated as follows.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \mathbf{w}^T \boldsymbol{\hat \mu} - \lambda \mathbf{w}^T \boldsymbol{\hat \Sigma} \mathbf{w} - \delta \|(\boldsymbol{S}^{1/2})^T \mathbf{w}\|_2 \\
&amp; \text{subject to}
&amp; &amp; \mathbf{1}^T \mathbf{w} = 1 \\
&amp; &amp; &amp; \mathbf{w} \geq \mathbf{0}
\end{aligned}
\]</span></p>
<p>Here, the term <span class="math inline">\(-\delta \|(\boldsymbol{S}^{1/2})^T \mathbf{w}\|_2\)</span> represents the worst-case deviation of the expected returns within the uncertainty ellipsoid, effectively hedging against the risk of underestimating the true returns.</p>
<p>This is a Second-Order Cone Programming (SOCP) formulation because it seeks to maximize a linear objective function subject to a mix of linear and conic constraints. The term <span class="math inline">\(- \delta \|(\boldsymbol{S}^{1/2})^T \mathbf{w}\|_2\)</span> within the objective function is representative of a conic constraint due to the inclusion of the <span class="math inline">\(L_2\)</span>-norm, which is typical in the standard form of an SOCP. Linear constraints are represented by the budget constraint <span class="math inline">\(\mathbf{1}^T \mathbf{w} = 1\)</span> and the non-negativity constraint <span class="math inline">\(\mathbf{w} \geq \mathbf{0}\)</span>.</p>
<p>The SOCP formulation is particularly powerful in portfolio optimization under uncertainty because it allows for a convex optimization problem that remains tractable even as the problem’s dimensionality (i.e., the number of assets) increases. Using the ellipsoidal uncertainty set for expected returns captures the essence of the estimation error in a more nuanced manner than a simple box constraint, allowing for correlation in the estimation errors across different assets.</p>
<p>Furthermore, the second-order conic constraints provide a way to model the risk of the portfolio in a robust manner. The term <span class="math inline">\(- \delta \|(\boldsymbol{S}^{1/2})^T \mathbf{w}\|_2\)</span> adds a penalty to the portfolio’s expected return proportional to the uncertainty in the returns, thus providing a hedge against the risk of underestimating the true returns. This penalty term, known as the robustness term, ensures that the optimized portfolio is not only tuned to the nominal estimates of the returns but also takes into account the worst-case deviations within the specified uncertainty set.</p>
</div>
</div>
</div>
</section>
<section id="spherical-uncertainty-on-the-covariance-matrix" class="level3">
<h3 class="anchored" data-anchor-id="spherical-uncertainty-on-the-covariance-matrix">Spherical Uncertainty on the Covariance Matrix</h3>
<p>This section introduces the Frobenius norm for integrating an aggregate uncertainty into the covariance matrix. This norm quantifies the size of the matrix as the square root of the sum of the squares of its elements, effectively encapsulating the matrix within a multidimensional spherical boundary. We use an additive perturbation matrix <span class="math inline">\(\Delta\)</span> to the input design matrix <span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{T \times N}\)</span>, where <span class="math inline">\(T\)</span> represents time periods and <span class="math inline">\(N\)</span> denotes the number of assets, thus:</p>
<p><span class="math display">\[
\boldsymbol{X} = \boldsymbol{\hat X} + \Delta
\]</span></p>
<p>Here, <span class="math inline">\(\Delta\)</span> is constrained in terms of its Frobenius norm as follows:</p>
<p><span class="math display">\[
\| \Delta \|_F \leq \delta_{\boldsymbol{X}}
\]</span></p>
<p>This formulation introduces a spherical uncertainty set, <span class="math inline">\(U_{\boldsymbol{X}}\)</span>, defined for the design matrix <span class="math inline">\(\boldsymbol{X}\)</span>:</p>
<p><span class="math display">\[
U_{\boldsymbol{X}} = \left\{ \boldsymbol{X} \mid \| \boldsymbol{X} - \boldsymbol{\hat X} \|_F \leq \delta_{\boldsymbol{X}} \right\}
\]</span></p>
<p>The uncertainty captured in the (demeaned) matrix <span class="math inline">\(\boldsymbol{X}\)</span> directly influences the sample covariance matrix <span class="math inline">\(\Sigma\)</span>, given by <span class="math inline">\(\Sigma = \frac{1}{T-1}\boldsymbol{X}^T\boldsymbol{X}\)</span>. This relation modifies the risk component in MVO as follows:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{w}^T \Sigma \mathbf{w} &amp;= \mathbf{w}^T \frac{1}{T-1}\boldsymbol{X}^T\boldsymbol{X} \mathbf{w} \\
&amp;= \frac{1}{T-1} (\boldsymbol{X} \mathbf{w})^T \boldsymbol{X} \mathbf{w} \\
&amp;= \frac{1}{T-1} \| \boldsymbol{X} \mathbf{w} \|_2^2 \\
&amp;= \frac{1}{T-1} \| (\boldsymbol{\hat X} + \Delta) \mathbf{w} \|_2^2
\end{aligned}
\]</span></p>
<p>Consequently, the robust MVO framework seeks to maximize the worst-case risk term in the inner optimization:</p>
<p><span class="math display">\[
\underset{\boldsymbol{X} \in U_{\boldsymbol{X}}}{\text{max}} \| \boldsymbol{X} \mathbf{w} \|_2^2 =
\underset{\| \Delta \|_F \leq \delta_{\boldsymbol{X}}}{\text{max}} \| (\boldsymbol{\hat X} + \Delta) \mathbf{w} \|_2^2
\]</span></p>
<p>Ignoring the constant factor <span class="math inline">\(\frac{1}{T-1}\)</span>, the robust MVO model is reformulated as:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \boldsymbol{\mu}^T \mathbf{w} - \lambda \underset{\| \Delta \|_F \leq \delta_{\boldsymbol{X}}}{\text{max}} \| (\boldsymbol{\hat X} + \Delta) \mathbf{w} \|_2^2 \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; &amp; &amp; w_i \geq 0, \quad \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]</span></p>
<p>We can show that the robust MVO formulation is equivalent to adding a squared <span class="math inline">\(l_2\)</span> norm to the objective function.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \boldsymbol{\mu}^T \mathbf{w} - \lambda (\| \boldsymbol{\hat X} \mathbf{w} \|_2 + \delta_{\boldsymbol{X}} \|\mathbf{w} \|_2)^2 \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; &amp; &amp; w_i \geq 0, \quad \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proposition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Proposition</strong></p>
<p>The robust portfolio optimization problem with a spherical uncertainty set over the covariance matrix is equivalent to adding an <span class="math inline">\(l_2\)</span> norm penalty to the risk term in the objective function:</p>
<p><span class="math display">\[
\underset{\mathbf{w}}{\text{max}}
\boldsymbol{\mu}^T \mathbf{w} - \lambda \underset{\| \Delta \|_F \leq \delta_{\boldsymbol{X}}}{\text{max}} \| (\boldsymbol{\hat X} + \Delta) \mathbf{w} \|_2^2 = \underset{\mathbf{w}}{\text{max}}
\boldsymbol{\mu}^T \mathbf{w} - \lambda (\| \boldsymbol{\hat X} \mathbf{w} \|_2 + \delta_{\boldsymbol{X}} \|\mathbf{w} \|_2)^2
\]</span></p>
<p><strong>Proof</strong></p>
<p>Using the triangular inequality, we deduce:</p>
<p><span class="math display">\[
\| (\boldsymbol{\hat X} + \Delta) \mathbf{w} \|_2 \leq \| \boldsymbol{\hat X} \mathbf{w} \|_2 +  \| \Delta \mathbf{w} \|_2
\]</span></p>
<p>Using the norm inequality, it follows that:</p>
<p><span class="math display">\[
\| \Delta \mathbf{w} \|_2 \leq \| \Delta \|_F \|\mathbf{w} \|_2 \leq \delta_{\boldsymbol{X}} \|\mathbf{w} \|_2
\]</span></p>
<p>Thus, the expression for the worst-case risk maximization simplifies to:</p>
<p><span class="math display">\[
\underset{\| \Delta \|_F \leq \delta_{\boldsymbol{X}}}{\text{max}} \| (\boldsymbol{\hat X} + \Delta) \mathbf{w} \|_2^2 = (\| \boldsymbol{\hat X} \mathbf{w} \|_2 + \delta_{\boldsymbol{X}} \|\mathbf{w} \|_2)^2
\]</span></p>
<p>Accordingly, the refined robust MVO framework is presented as follows.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \boldsymbol{\mu}^T \mathbf{w} - \lambda (\| \boldsymbol{\hat X} \mathbf{w} \|_2 + \delta_{\boldsymbol{X}} \|\mathbf{w} \|_2)^2 \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; &amp; &amp; w_i \geq 0, \quad \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]</span></p>
<p>This formulation represents a Second-Order Cone Programming (SOCP) problem, offering a structured approach to addressing uncertainty in portfolio optimization through the lens of the Frobenius norm.</p>
<p>Note that the original MVO formulation can be expressed as follows.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{maximize}}
&amp; &amp; \boldsymbol{\mu}^T \mathbf{w} - \lambda \| \boldsymbol{\hat X} \mathbf{w} \|_2^2 \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; &amp; &amp; w_i \geq 0, \quad \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]</span></p>
<p>Comparing the expressions, we see that the extra term <span class="math inline">\(\delta_{\boldsymbol{X}} \|\mathbf{w} \|_2\)</span> serves as an additional regularization term in the robust formulation, playing a similar role as the <span class="math inline">\(L_2\)</span> penalty. To see this, consider the following related global minimum risk portfolio:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \underset{\mathbf{w}}{\text{minimize}}
&amp; &amp;  \| \boldsymbol{\hat X} \mathbf{w} \|_2^2 +  \delta_{\boldsymbol{X}} \|\mathbf{w} \|_2^2 \\
&amp; \text{subject to}
&amp; &amp; \mathbf{w}^T \mathbf{1} = 1, \\
&amp; &amp; &amp; w_i \geq 0, \quad \forall i \in \{1, \ldots, n\}.
\end{aligned}
\]</span></p>
<p>The objective is equivalent to placing an <span class="math inline">\(L_2\)</span> norm on the weights via ridge regression:</p>
<p><span class="math display">\[
\begin{aligned}
\| \boldsymbol{\hat X} \mathbf{w} \|_2^2 +  \delta_{\boldsymbol{X}} \|\mathbf{w} \|_2^2  &amp;=  \mathbf{w}^T \boldsymbol{\hat X}^T \boldsymbol{\hat X} \mathbf{w} + \delta_{\boldsymbol{X}} \mathbf{w}^T \mathbf{w} \\
&amp;= \mathbf{w}^T (\boldsymbol{\hat X}^T \boldsymbol{\hat X} + \delta_{\boldsymbol{X}} \mathbf{I}) \mathbf{w}
\end{aligned}
\]</span></p>
<p>which is equivalent to regularizing the (unscaled) sample covariance matrix in ridge regression.</p>
</div>
</div>
</div>
</section>
</section>
<section id="integrating-gbl-with-robust-optimization" class="level2">
<h2 class="anchored" data-anchor-id="integrating-gbl-with-robust-optimization">Integrating GBL with Robust Optimization</h2>
<p>While the GBL model provides improved estimates of expected returns and covariances by accounting for biases, there remains inherent uncertainty in these estimates due to model risk and estimation errors. Traditional MVO uses point estimates for expected returns and covariances, which can lead to suboptimal portfolios if these estimates are inaccurate. Robust optimization addresses this issue by optimizing the portfolio under worst-case scenarios within specified uncertainty sets.</p>
<section id="defining-uncertainty-sets-from-gbl-posterior-distribution" class="level3">
<h3 class="anchored" data-anchor-id="defining-uncertainty-sets-from-gbl-posterior-distribution">Defining Uncertainty Sets from GBL Posterior Distribution</h3>
<p>The posterior distribution obtained from the GBL model is:</p>
<p><span class="math display">\[
\mathbf{r}_t | \mathcal{V}_t, \mathcal{R}_{t-1} \sim \mathcal{N}(\boldsymbol{\mu}_{GBL}, \Sigma_{GBL})
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu}_{GBL}\)</span> and <span class="math inline">\(\Sigma_{GBL}\)</span> are the posterior mean and covariance matrix estimated via Gibbs sampling.</p>
<p>We can define an uncertainty set for the expected returns based on confidence intervals derived from the posterior distribution. For example, using a confidence level <span class="math inline">\(\kappa \in (0,1)\)</span>, the uncertainty set <span class="math inline">\(\mathcal{U}\)</span> can be defined as:</p>
<p><span class="math display">\[
\mathcal{U} = \left\{ \boldsymbol{\mu} : (\boldsymbol{\mu} - \boldsymbol{\mu}_{GBL})^\top \Sigma_{GBL}^{-1} (\boldsymbol{\mu} - \boldsymbol{\mu}_{GBL}) \leq \chi^2_{N}(\kappa) \right\}
\]</span></p>
<p>where <span class="math inline">\(\chi^2_{N}(\kappa)\)</span> is the chi-squared distribution critical value with <span class="math inline">\(N\)</span> degrees of freedom at confidence level <span class="math inline">\(\kappa\)</span>.</p>
</section>
<section id="robust-portfolio-optimization-problem" class="level3">
<h3 class="anchored" data-anchor-id="robust-portfolio-optimization-problem">Robust Portfolio Optimization Problem</h3>
<p>The robust Mean-Variance Optimization problem aims to find the portfolio weights <span class="math inline">\(\mathbf{w}\)</span> that maximize the worst-case expected return while considering the uncertainty in expected returns:</p>
<p><span class="math display">\[
\begin{align*}
\max_{\mathbf{w}} &amp; \quad \min_{\boldsymbol{\mu} \in \mathcal{U}} \boldsymbol{\mu}^\top \mathbf{w} - \frac{\gamma}{2} \mathbf{w}^\top \Sigma_{GBL} \mathbf{w} \\
\text{subject to} &amp; \quad \mathbf{w}^\top \mathbf{1} = 1 \\
&amp; \quad \mathbf{w} \geq \mathbf{0}
\end{align*}
\]</span></p>
<p>This optimization problem seeks the portfolio that performs best under the worst-case expected return within the uncertainty set <span class="math inline">\(\mathcal{U}\)</span>.</p>
<p>The inner minimization problem can be solved analytically due to the convexity of the uncertainty set and the linearity of the objective function in <span class="math inline">\(\boldsymbol{\mu}\)</span>. The worst-case expected return occurs at the minimal point of <span class="math inline">\(\boldsymbol{\mu}\)</span> within <span class="math inline">\(\mathcal{U}\)</span> along the direction of <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p>The inner minimization becomes:</p>
<p><span class="math display">\[
\min_{\boldsymbol{\mu} \in \mathcal{U}} \boldsymbol{\mu}^\top \mathbf{w} = \boldsymbol{\mu}_{GBL}^\top \mathbf{w} - \sqrt{\chi^2_{N}(\kappa)} \| \Sigma_{GBL}^{1/2} \mathbf{w} \|
\]</span></p>
<p>Substituting back into the optimization problem:</p>
<p><span class="math display">\[
\max_{\mathbf{w}} \quad \boldsymbol{\mu}_{GBL}^\top \mathbf{w} - \sqrt{\chi^2_{N}(\kappa)} \| \Sigma_{GBL}^{1/2} \mathbf{w} \| - \frac{\gamma}{2} \mathbf{w}^\top \Sigma_{GBL} \mathbf{w}
\]</span></p>
<p>This problem can be reformulated as a second-order cone programming (SOCP) problem, which can be efficiently solved using modern optimization solvers.</p>

</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-black1991" class="csl-entry" role="listitem">
Black, Fischer, and Robert B Litterman. 1991. <span>“Asset Allocation.”</span> <em>The Journal of Fixed Income</em> 1 (2): 7–18. <a href="https://doi.org/10.3905/jfi.1991.408013">https://doi.org/10.3905/jfi.1991.408013</a>.
</div>
<div id="ref-chen2020" class="csl-entry" role="listitem">
Chen, Shea D., and Andrew E. B. Lim. 2020. <span>“A Generalized Black<span></span>Litterman Model.”</span> <em>Operations Research</em>, January. <a href="https://doi.org/10.1287/opre.2019.1893">https://doi.org/10.1287/opre.2019.1893</a>.
</div>
<div id="ref-fama2003" class="csl-entry" role="listitem">
Fama, Eugene F., and Kenneth R. French. 2003. <span>“The Capital Asset Pricing Model: Theory and Evidence.”</span> <em>SSRN Electronic Journal</em>. <a href="https://doi.org/10.2139/ssrn.440920">https://doi.org/10.2139/ssrn.440920</a>.
</div>
<div id="ref-sra2012optimization" class="csl-entry" role="listitem">
Sra, Suvrit, Sebastian Nowozin, and Stephen J Wright. 2012. <em>Optimization for Machine Learning</em>. Mit Press.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>We assume without loss of generality that the risk-free rate is zero. If the risk-free rate is nonzero, simply replace <span class="math inline">\(\mathbf{r}\)</span> with the vector of excess returns and our analysis carries through.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>